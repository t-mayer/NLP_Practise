{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "CNN_for_NLP.ipynb",
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9CbHo82i3WG",
        "outputId": "f1e858d8-2ba1-4ec7-868f-7609badde481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXcVbYVUenKh",
        "outputId": "58dde120-d4c4-47c3-f159-7c613a216d00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tarfile\n",
        "from tensorflow.compat.v1.keras import backend as K\n",
        "from keras.preprocessing import sequence # helper module to handle padding\n",
        "from keras.models import Sequential # the model\n",
        "from keras.layers import Dense, Dropout, Activation # layer objects\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D # convolution layer and pooling\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from random import shuffle\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "\n",
        "# load pre-trained google news vectors\n",
        "word2vec_path = '/content/drive/My Drive/machine_learning/train/GoogleNews-vectors-negative300.bin.gz'\n",
        "word_vectors = KeyedVectors.load_word2vec_format(datapath(word2vec_path), binary=True, limit=200000)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b98jqfVaenKs"
      },
      "source": [
        "# load the data\n",
        "data = '/content/drive/My Drive/machine_learning/train'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBf7ne4DenK3"
      },
      "source": [
        "\n",
        "# preprocess movie reviews\n",
        "def preprocess_data(filepath):\n",
        "    # join paths\n",
        "    positive_path = os.path.join(filepath, 'pos/pos')\n",
        "    negative_path = os.path.join(filepath, 'neg/neg')\n",
        "    \n",
        "    # create labels\n",
        "    pos_label = 1\n",
        "    neg_label = 0\n",
        "    \n",
        "    # make list for dataset\n",
        "    dataset = []\n",
        "    \n",
        "    # use glob to find all the pathnames matching a specified pattern\n",
        "    # read each txt-file in the the folder for positive reviews\n",
        "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
        "        \n",
        "        # append review and label as tuple to dataset \n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((pos_label, f.read()))\n",
        "    \n",
        "    # do the same for the negative reviews\n",
        "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((neg_label, f.read()))  \n",
        "            \n",
        "     # shuffle position of positive + negative reviews\n",
        "    shuffle(dataset)\n",
        "    return dataset\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkJUQ9kJenK_"
      },
      "source": [
        "# tokenize + transform to word vectors    \n",
        "def tokenize_and_vectorize(dataset):\n",
        "    # tokenizer object\n",
        "    tokenizer = TreebankWordTokenizer()\n",
        "    \n",
        "    # list to store vectorized data\n",
        "    vectorized_data = []\n",
        "    \n",
        "    # iterate over reviews\n",
        "    for sample in dataset:\n",
        "        \n",
        "        # tokenize the review\n",
        "        tokens = tokenizer.tokenize(sample[1])\n",
        "        \n",
        "        # make list to store \n",
        "        token_vectors = []\n",
        "        \n",
        "        # iterate over the token list of each review\n",
        "        for token in tokens:\n",
        "            \n",
        "            # look up the word vector and append to token_vectors list\n",
        "            try:\n",
        "                token_vectors.append(word_vectors[token])\n",
        "            except KeyError:\n",
        "                pass # No matching token in the Google w2v vocab\n",
        "       \n",
        "        # now append the vectorized review to a common list vectorized_data\n",
        "        vectorized_data.append(token_vectors)\n",
        "\n",
        "    return vectorized_data\n",
        "            \n",
        "    \n",
        "   "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhXomKoEenLG"
      },
      "source": [
        "# collect target values in the same order as reviews\n",
        "def collect_label(dataset):\n",
        "    \n",
        "    # make file to store neg or pos label\n",
        "    label = []\n",
        "    \n",
        "    # iterate over dataset and get the label\n",
        "    for sample in dataset:\n",
        "        # append\n",
        "        label.append(sample[0])\n",
        "        \n",
        "    return label"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHKXn2NkenLN"
      },
      "source": [
        "# use functions on dataset\n",
        "\n",
        "# returns list of tuples (1/0, review)\n",
        "preprocessed_data = preprocess_data(data)\n",
        "\n",
        "# takes the list of tuples, vectorizes it, returns list of vectors\n",
        "vectorized_data = tokenize_and_vectorize(preprocessed_data)\n",
        "\n",
        "# returns list of labels\n",
        "expected_label = collect_label(preprocessed_data)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-WjzQJqenLU"
      },
      "source": [
        "# train/test split\n",
        "\n",
        "# split point: # 80%-20% split\n",
        "split_point = int(len(vectorized_data)*.8) \n",
        "\n",
        "# 80% train data: x and y label\n",
        "x_train = vectorized_data[:split_point]\n",
        "y_train = expected_label[:split_point]\n",
        "\n",
        "# the rest (20%) test data: x and y label\n",
        "x_test = vectorized_data[split_point:]\n",
        "y_test = expected_label[split_point:]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67NdadC_enLa"
      },
      "source": [
        "# set CNN parameters\n",
        "maxlen = 400  # length of reviews: 400 tokens (trunacte longer, pad shorter)\n",
        "batch_size = 30 # how many samples to process before backprop\n",
        "embedding_dims = 300 # length of the token vectors to pass to convnet\n",
        "filters = 250 # num of filters to train\n",
        "kernel_size = 3 # width of the filter (three tokens)\n",
        "hidden_dims = 250 # num of neurons in the feedforward net at the end\n",
        "epochs = 30 # num of times to pass the entire train set to network"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJn3m-B5enLe"
      },
      "source": [
        "# pad and truncate token sequence (seqs of vectors)\n",
        "# note: pad_sequences by keras can only be used for seqs of scalars\n",
        "\n",
        "def pad_or_truncate(data, maxlen):\n",
        "    \n",
        "    # make list to store padded/truncated data\n",
        "    new_data = []\n",
        "    \n",
        "    # Create a vector of 0s the length of the word vectors\n",
        "    zero_vector = []\n",
        "    for _ in range(len(data[0][0])): # append 300 0s to word vector\n",
        "        zero_vector.append(0.0)\n",
        "        \n",
        "   # iterate over list of vectors     \n",
        "    for sample in data:\n",
        "        \n",
        "        # if word vector longer than 400\n",
        "        if len(sample) > maxlen:\n",
        "            # cut from 0 to 400\n",
        "            temp = sample[:maxlen]\n",
        "            \n",
        "        # if word vector smaller than 400\n",
        "        elif len(sample) < maxlen:\n",
        "            # leave as it is \n",
        "            temp = sample\n",
        "            \n",
        "            #  Pad by appending 0 vectors to the list\n",
        "            additional_elems = maxlen - len(sample)\n",
        "            \n",
        "            for _ in range(additional_elems):\n",
        "                temp.append(zero_vector)\n",
        "                \n",
        "        # if word vector is exactly 400, leave as it is \n",
        "        else:\n",
        "            temp = sample\n",
        "        \n",
        "        # append the padded/truncated vector to list new_data\n",
        "        new_data.append(temp)\n",
        "            \n",
        "    return new_data\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfQN9UiYenLl"
      },
      "source": [
        "# pad or truncate, returns a list of equally-sized word vectors\n",
        "x_train = pad_or_truncate(x_train, maxlen)\n",
        "x_test = pad_or_truncate(x_test, maxlen)\n",
        "\n",
        "# reshape to arrays\n",
        "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
        "y_test = np.array(y_test)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcoEarIxenLq"
      },
      "source": [
        "# build model\n",
        "model = Sequential() # base class\n",
        "\n",
        "# add convolutional layer\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size, # window width = 3 tokens\n",
        "                 padding='valid', # valid padding\n",
        "                 activation='relu',\n",
        "                 strides=1, # shift of one token\n",
        "                 input_shape=(maxlen, embedding_dims)))\n",
        "\n",
        "# add max pooling: take largest activation value for the given region\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# add a fully-connected dense layer with dropout\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "#  single-unit output layer\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPhOZ8wjenLw"
      },
      "source": [
        "# compile builds the model\n",
        "model.compile(loss='binary_crossentropy', # binary because only one output neuron\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ9WG0HQenL1",
        "outputId": "a17f296e-cc48-4606-88ea-29cd193c19c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# fit trains the model\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=2,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "118/118 - 38s - loss: 0.5531 - accuracy: 0.6991 - val_loss: 0.4134 - val_accuracy: 0.8000\n",
            "Epoch 2/30\n",
            "118/118 - 37s - loss: 0.2658 - accuracy: 0.8919 - val_loss: 0.3040 - val_accuracy: 0.8864\n",
            "Epoch 3/30\n",
            "118/118 - 37s - loss: 0.0940 - accuracy: 0.9764 - val_loss: 0.2950 - val_accuracy: 0.8807\n",
            "Epoch 4/30\n",
            "118/118 - 37s - loss: 0.0190 - accuracy: 0.9991 - val_loss: 0.3187 - val_accuracy: 0.8818\n",
            "Epoch 5/30\n",
            "118/118 - 36s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.3306 - val_accuracy: 0.8841\n",
            "Epoch 6/30\n",
            "118/118 - 37s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.3450 - val_accuracy: 0.8864\n",
            "Epoch 7/30\n",
            "118/118 - 36s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3560 - val_accuracy: 0.8830\n",
            "Epoch 8/30\n",
            "118/118 - 37s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3665 - val_accuracy: 0.8830\n",
            "Epoch 9/30\n",
            "118/118 - 37s - loss: 7.6675e-04 - accuracy: 1.0000 - val_loss: 0.3751 - val_accuracy: 0.8841\n",
            "Epoch 10/30\n",
            "118/118 - 37s - loss: 5.8240e-04 - accuracy: 1.0000 - val_loss: 0.3802 - val_accuracy: 0.8875\n",
            "Epoch 11/30\n",
            "118/118 - 38s - loss: 4.5206e-04 - accuracy: 1.0000 - val_loss: 0.3880 - val_accuracy: 0.8875\n",
            "Epoch 12/30\n",
            "118/118 - 38s - loss: 3.5764e-04 - accuracy: 1.0000 - val_loss: 0.3959 - val_accuracy: 0.8841\n",
            "Epoch 13/30\n",
            "118/118 - 40s - loss: 2.9589e-04 - accuracy: 1.0000 - val_loss: 0.4013 - val_accuracy: 0.8852\n",
            "Epoch 14/30\n",
            "118/118 - 40s - loss: 2.5112e-04 - accuracy: 1.0000 - val_loss: 0.4063 - val_accuracy: 0.8864\n",
            "Epoch 15/30\n",
            "118/118 - 40s - loss: 2.0548e-04 - accuracy: 1.0000 - val_loss: 0.4130 - val_accuracy: 0.8864\n",
            "Epoch 16/30\n",
            "118/118 - 38s - loss: 1.7428e-04 - accuracy: 1.0000 - val_loss: 0.4188 - val_accuracy: 0.8852\n",
            "Epoch 17/30\n",
            "118/118 - 38s - loss: 1.5252e-04 - accuracy: 1.0000 - val_loss: 0.4243 - val_accuracy: 0.8852\n",
            "Epoch 18/30\n",
            "118/118 - 38s - loss: 1.2786e-04 - accuracy: 1.0000 - val_loss: 0.4272 - val_accuracy: 0.8886\n",
            "Epoch 19/30\n",
            "118/118 - 38s - loss: 1.1808e-04 - accuracy: 1.0000 - val_loss: 0.4340 - val_accuracy: 0.8852\n",
            "Epoch 20/30\n",
            "118/118 - 39s - loss: 9.9280e-05 - accuracy: 1.0000 - val_loss: 0.4374 - val_accuracy: 0.8864\n",
            "Epoch 21/30\n",
            "118/118 - 38s - loss: 8.8783e-05 - accuracy: 1.0000 - val_loss: 0.4420 - val_accuracy: 0.8886\n",
            "Epoch 22/30\n",
            "118/118 - 37s - loss: 7.9930e-05 - accuracy: 1.0000 - val_loss: 0.4480 - val_accuracy: 0.8886\n",
            "Epoch 23/30\n",
            "118/118 - 37s - loss: 6.2530e-05 - accuracy: 1.0000 - val_loss: 0.4550 - val_accuracy: 0.8886\n",
            "Epoch 24/30\n",
            "118/118 - 36s - loss: 5.0403e-05 - accuracy: 1.0000 - val_loss: 0.4610 - val_accuracy: 0.8864\n",
            "Epoch 25/30\n",
            "118/118 - 39s - loss: 4.1055e-05 - accuracy: 1.0000 - val_loss: 0.4696 - val_accuracy: 0.8898\n",
            "Epoch 26/30\n",
            "118/118 - 38s - loss: 3.6827e-05 - accuracy: 1.0000 - val_loss: 0.4772 - val_accuracy: 0.8898\n",
            "Epoch 27/30\n",
            "118/118 - 38s - loss: 3.2117e-05 - accuracy: 1.0000 - val_loss: 0.4842 - val_accuracy: 0.8898\n",
            "Epoch 28/30\n",
            "118/118 - 37s - loss: 2.7359e-05 - accuracy: 1.0000 - val_loss: 0.4908 - val_accuracy: 0.8898\n",
            "Epoch 29/30\n",
            "118/118 - 38s - loss: 2.2892e-05 - accuracy: 1.0000 - val_loss: 0.4966 - val_accuracy: 0.8875\n",
            "Epoch 30/30\n",
            "118/118 - 39s - loss: 2.1538e-05 - accuracy: 1.0000 - val_loss: 0.5029 - val_accuracy: 0.8852\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f13f39045c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxVGKuB4enL9"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}